---
layout: post
title: Weekly 005
subtitle: "Algorithm:Construct Binary Tree from Preorder and Inorder Traversal; Review: ; Tips:Srping Boot import external jar ; Share: "
tags: [it]
category: it
comments: true
---

# Table of Contents

1.  [Algorithm](#org8757156)
2.  [Review](#org1c162af)
3.  [Tips](#orge8631ad)
4.  [Share](#orge448f16)

<a id="org8757156"></a>

# Algorithm

Leetcode 105: [Construct Binary Tree from Preorder and Inorder Traversal](https://leetcode.com/problems/construct-binary-tree-from-preorder-and-inorder-traversal/description/)

![algorithm desc](/assets/img/week05-alg.png)

```java
class Solution {

    private Map<Integer, Integer> valToIndex = new HashMap<>();

    public TreeNode buildTree(int[] preorder, int[] inorder) {
        if(preorder == null || inorder == null) return null;

        for(int i = 0; i < inorder.length; i++){
            valToIndex.put(inorder[i], i);
        }

        TreeNode root = build(preorder, 0, preorder.length - 1, inorder, 0, inorder.length - 1);
        return root;
    }

    TreeNode build(int[] preorder, int preStart, int preEnd,
               int[] inorder, int inStart, int inEnd) {

        if (preStart > preEnd || inStart > inEnd) {
            return null;
        }

        // root 节点对应的值就是前序遍历数组的第一个元素
        int rootVal = preorder[preStart];
        // rootVal 在中序遍历数组中的索引
        int index = valToIndex.get(rootVal);

        int leftSize = index - inStart;

        // 先构造出当前根节点
        TreeNode root = new TreeNode(rootVal);
        // 递归构造左右子树
        root.left = build(preorder, preStart + 1, preStart + leftSize,
                        inorder, inStart, index - 1);

        root.right = build(preorder, preStart + leftSize + 1, preEnd,
                        inorder, index + 1, inEnd);
        return root;
    }
}
```

<a id="org1c162af"></a>

# Review

<a id="orge8631ad"></a>

# Tip

The “it” in AI models is the dataset.[原文](https://nonint.com/2023/06/10/the-it-in-ai-models-is-the-dataset/)

我在 OpenAI 工作已经快一年了。这段时间里，我训练了很多生成式 AI 模型，比任何人能想到的还要多。

每当我花了几个小时，观察和调整各种模型配置和参数时，有一件事让我印象深刻，那就是所有训练结果之间的相似性。

我越来越发现，这些模型以令人难以置信的程度，向它们的语料集靠近。

这表明在相同的语料集上训练足够长的时间，几乎每个具有足够权重和训练时间的模型都会收敛到同一点。足够大的扩散卷积网络会产生相同的结果。

这是一个令人惊讶的观察！

这意味着模型行为不是由架构、参数或优化器决定的。它由你的语料集决定，没有其他决定因素。其他一切因素都不过是为了有效计算以近似该语料集的手段。

当你谈论 Lambda、ChatGPT、Bard 或 Claude 时，指的并不是它们的模型，而是它们的语料集。

<a id="orge448f16"></a>

# Share
